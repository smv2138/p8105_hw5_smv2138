---
title: "Homework 5"
output: github_document
author: Sushupta Vijapur (smv2138)
---
```{r}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

## all plots i make will have the viridis color palette
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

## Problem 1

```{r}
homicide_df = 
  read_csv("hom_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Let's look at this a bit

```{r}
aggregate_df = 
    homicide_df %>% 
      group_by(city_state) %>% 
      summarize(
        hom_total = n(),
        hom_unsolved = sum(resolved == "unsolved")
      )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)) %>% 
  broom::tidy()
```


Try to iterate
map2 gives you 2 inputs (hom unsolved and hom total)
```{r}
results_df = 
  aggregate_df %>% 
    mutate(
      prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
      tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
    ) %>% 
    select(-prop_tests) %>% 
    unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

Created a point estimate for the unsolved homicides by city state in the US

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


```{r}

homicide_df = 
  read_csv("hom_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```


## Problem 2

#### Part 1

Start with a dataframe containing all file names; the list.files function will help
Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
Tidy the result; manipulate file names to include control arm and subject ID, make sure weekly observations are “tidy”, and do any other tidying that’s necessary
```{r}
path_df = 
  tibble(
  path = list.files("long_data")
  ) %>% 
  mutate(path = str_c("long_data/", path),
         path_expand = map(.x = path, ~read_csv(.x))
  ) %>% 
  unnest(path_expand) %>% 
  mutate(
    id_arm = str_sub(path, -10, -5)
    ) %>% 
  relocate(id_arm)
 
         
```

#### Part 2

Make a spaghetti plot showing observations on each subject over time, and comment on differences between groups.
Seperate id and arm and group by id 
```{r}
path_df %>% 
  separate(id_arm, c("arm", "id"), convert = TRUE) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "measurements"
  ) %>%
  group_by(id) %>% 
  ggplot(aes(x = week, y = measurements, group = arm)) +
    geom_smooth(aes(color = arm), se = FALSE) +
  labs(
    title = "Observations overtime by treatment group",
    x = "Week",
    y = "Measurements"
  )
```

The experimental groups have higher measurements compared to the control groups. It also seems as though the measurements for the experimental groups are increasing over time while the measurements for the control groups remain steady and decreasing a little over time.

## Problem 3

#### Part 1

Simulating data
```{r}
t_test  = function(samp_size = 30, mu, sigma = 5) {
      data = 
        tibble(
          
          x = rnorm(n = samp_size, mean = mu, sd = sigma)
        )
      
        data %>% 
        t.test() %>% 
          broom::tidy()
 }

data =
    tibble(
      mu = c(0, 1, 2, 3, 4, 5, 6)
    ) %>% 
          mutate(
            output = map(.x = mu, ~ rerun(5000, t_test(mu = .x))),
            results = map(output, bind_rows),
            true_mu = mu,
          ) %>% 
          select(results, true_mu) %>% 
          unnest(results) %>% 
          select(estimate, p.value, true_mu) %>% 
          mutate(
                decision = ifelse(p.value > 0.05, "fail_reject", "reject")
          )


```

#### Part 2

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.
```{r}
plot1  = 
  data %>% 
  group_by(true_mu) %>% 
  summarize(
    decision_tot = n(),
    decision_reject = sum(decision == "reject")
  ) %>% 
  mutate(
    prop = map2(.x = decision_reject, .y = decision_tot, ~prop.test(x = .x, n = .y)),
    prop_tidy = map(.x = prop, ~broom::tidy(.x))
  ) %>% 
  select(-prop) %>% 
  unnest(prop_tidy) %>% 
  select(true_mu, estimate) %>% 
  rename(power = estimate)

```

Plot 1
```{r}
plot1 %>% 
  ggplot(aes(x = true_mu, y = power)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  labs(
    title = "Effect Size by Power",
    x = "Effect Size",
    y = "Power"
  )

```

Based on this graph we can see that as the effect size increases, power increases. This means that as the effect size is getting further away from the null value of 0, the probability that the test will correctly reject the null hypothesis is also increasing. 

#### Part 3

Make a plot showing the average estimate of μ^ on the y axis and the true value of μ on the x axis. 

```{r}
all = 
 data %>% 
    group_by(true_mu) %>% 
    summarize(
      all_mean = mean(estimate)
    )

all %>%   
  ggplot(aes(x = true_mu, y = all_mean)) +
    geom_point() +
    geom_smooth(se = FALSE) +
    labs(
      title = "Comparing true mu to average estiamte mu among all tests",
      x = "True mu value",
      y = "Average estimated mu value"
    )
  
```


Make a second plot (or overlay on the first) the average estimate of μ^ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. Is the sample average of μ^ across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

```{r}
rejected = 
  data %>% 
    filter(decision == "reject") %>% 
    group_by(true_mu) %>% 
    summarize(
      rejected_mean = mean(estimate)
    )
```

```{r}
combined = 
  left_join(all, rejected, by = "true_mu") %>% 
  pivot_longer(
    all_mean:rejected_mean,
    names_to = "mean_type",
    values_to = "avg_estimated_mu"
  )

combined %>% 
  ggplot(aes(x = true_mu, y = avg_estimated_mu, color = mean_type)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    title = "Comparing true mu and average estimate mu for all tests and rejected tests",
    x = "True mu value",
    y = "Average estimated mu value"
  )
```

The average estimated my across tests where the null hypothesis was rejected did not equal the true value of mu until the true value approached approximately 4. When the true mu was less than 4, the estimated mu was larger among the rejected tests was larger than the estimated mu among all tests (except when the true mu was 0). Therefore, we can see when we only include those tests which were rejected, the estimated mu is not close to the true mu. 

